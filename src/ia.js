const OpenAI = require('openai');
const PROMPT_BASE = require('./prompt');

const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

// üëâ TEXTO DE SEGURAN√áA (fallback absoluto)
const RESPOSTA_PADRAO_FORA_ESCOPO =
  'Posso te ajudar com produtos, pre√ßos ou o cat√°logo da Alum√≠nio JR.';

// üëâ FRASE PARA FALTA DE DADOS
const RESPOSTA_FALTA_INFO =
  'Me diga o nome exato do produto e o tamanho ou litragem.';

// üëâ LINK DO CAT√ÅLOGO (CONTROLADO PELO BACKEND)
const LINK_CATALOGO = 'https://SEU_LINK_DE_CATALOGO_AQUI';

// üëâ DADOS DO CAT√ÅLOGO (TEMPOR√ÅRIO)
const CATALOGO_DADOS = [
  'Panela de Press√£o 3L',
  'Panela de Press√£o 4,5L',
  'Ca√ßarola Alum√≠nio 20',
  'Ca√ßarola Alum√≠nio 24',
  'Cafeteira Alum√≠nio 1L'
];

/**
 * Fun√ß√£o para gerar resposta usando IA
 * @param {string} textoCliente - A mensagem atual do usu√°rio
 * @param {Array} historico - Array de mensagens anteriores vindo do app.js
 */
async function responderComIA(textoCliente, historico = []) {
  try {
    const promptFinal = PROMPT_BASE
      .replace('{{LINK_CATALOGO}}', LINK_CATALOGO)
      .replace('{{CATALOGO_DADOS}}', CATALOGO_DADOS.join(', '));

    // Montagem do Chat Completion padr√£o OpenAI
    const response = await client.chat.completions.create({
      model: 'gpt-4o-mini', // Nome correto do modelo (mini e econ√¥mico)
      messages: [
        {
          role: 'system',
          content: promptFinal
        },
        ...historico, // Insere as mensagens anteriores para contexto
        {
          role: 'user',
          content: textoCliente
        }
      ],
      temperature: 0,
      max_tokens: 150 // Nome correto do par√¢metro de limite de sa√≠da
    });

    const resposta = response.choices[0]?.message?.content;

    if (!resposta) {
      return RESPOSTA_PADRAO_FORA_ESCOPO;
    }

    return resposta.trim();

  } catch (err) {
    console.error('‚ùå ERRO OPENAI:', err.message);
    return RESPOSTA_PADRAO_FORA_ESCOPO;
  }
}

module.exports = {
  responderComIA
};
